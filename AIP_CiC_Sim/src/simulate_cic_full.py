
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CiC full simulator
- Input: filteredCorpus.csv (Colors in Context trial-level CSV)
- Output: Table 1 (ablation), Table 2 (sensitivity), Figures 1â€“6
Author: (autogenerated)
"""

import argparse, pathlib, sys, math, json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# -----------------------------
# Utilities
# -----------------------------

def angular_dist(a, b):
    """Distance on unit circle for hue (0..1)."""
    d = abs(a-b) % 1.0
    return min(d, 1.0-d)

def hsl_to_rgb(h, s, l):
    # standard HSL->RGB conversion (0..1 range)
    def hue2rgb(p, q, t):
        if t < 0: t += 1
        if t > 1: t -= 1
        if t < 1/6: return p + (q - p) * 6 * t
        if t < 1/2: return q
        if t < 2/3: return p + (q - p) * (2/3 - t) * 6
        return p
    if s == 0:
        r = g = b = l
    else:
        q = l * (1 + s) if l < 0.5 else l + s - l*s
        p = 2*l - q
        r = hue2rgb(p, q, h + 1/3)
        g = hue2rgb(p, q, h)
        b = hue2rgb(p, q, h - 1/3)
    return r, g, b

def wilson_from_counts(successes, n, z=1.96):
    if n == 0:
        return np.nan, np.nan, np.nan
    p = successes / n
    denom = 1 + z*z/n
    centre = p + z*z/(2*n)
    pm = z*np.sqrt((p*(1-p) + z*z/(4*n))/n)
    lo = (centre - pm) / denom
    hi = (centre + pm) / denom
    return p, lo, hi

def mean_and_se(arr):
    if len(arr) == 0: return np.nan, np.nan
    a = np.array(arr, float)
    return float(a.mean()), float(a.std(ddof=1)/max(1, np.sqrt(len(a))))

# -----------------------------
# Data loader
# -----------------------------

def load_cic_csv(csv_path):
    """
    Minimal loader for CiC filteredCorpus.csv.

    Expected columns (best effort, flexible):
      - HSL per option: clickColH/S/L, alt1ColH/S/L, alt2ColH/S/L  (0..360 for H; 0..1 or 0..100 for S/L)
      - OR RGB per option: clickR/G/B, alt1R/G/B, alt2R/G/B (0..255)
      - Target label via *Status columns*: clickStatus/alt1Status/alt2Status ('target' or 1), or target_index in {0,1,2}
      - Optional 'condition' in {'close','far','split'}

    Returns:
      colors_hsl: (N,3,3) array with H,S,L in 0..1
      targets: (N,) int indices 0..2
      conds: list[str] length N
    """
    df = pd.read_csv(csv_path)
    cols = df.columns.str.lower().tolist()

    # --- condition ---
    cond = None
    for c in ['condition','cond']:
        if c in df.columns.str.lower():
            cond = df[df.columns[df.columns.str.lower()==c][0]].astype(str).str.lower().tolist()
            break
    if cond is None:
        cond = ['all']*len(df)

    # --- target index ---
    target_idx = None
    # via status columns
    status_cols = [('clickstatus','alt1status','alt2status'), ('click_status','alt1_status','alt2_status')]
    for sc in status_cols:
        if all(s in cols for s in sc):
            s0 = df[df.columns[cols.index(sc[0])]].astype(str).str.lower()
            s1 = df[df.columns[cols.index(sc[1])]].astype(str).str.lower()
            s2 = df[df.columns[cols.index(sc[2])]].astype(str).str.lower()
            t = []
            for a,b,c in zip(s0,s1,s2):
                if a in ('target','gold','1','true','t'): t.append(0)
                elif b in ('target','gold','1','true','t'): t.append(1)
                elif c in ('target','gold','1','true','t'): t.append(2)
                else: t.append(0)  # fallback
            target_idx = np.array(t, int)
            break
    if target_idx is None:
        # direct index
        for name in ['target_index','target','gold_index','answer_index']:
            if name in df.columns.str.lower():
                target_idx = df[df.columns[df.columns.str.lower()==name][0]].astype(int).to_numpy()
                break
    if target_idx is None:
        raise SystemExit("Could not determine target indices. Provide status columns or target_index.")

    # --- colors ---
    def get_triplet(prefix):
        return [prefix+'colh', prefix+'cols', prefix+'coll'], [prefix+'r',prefix+'g',prefix+'b']

    def columns_exist(names):
        return all(n in cols for n in names)

    hsl_names = []
    rgb_names = []
    for p in ['click','alt1','alt2']:
        h_names, r_names = get_triplet(p)
        if columns_exist(h_names):
            hsl_names.append(h_names)
        elif columns_exist(r_names):
            rgb_names.append(r_names)
        else:
            # try capitalized variants
            h_names = [p+'colh', p+'cols', p+'coll']
            r_names = [p+'r', p+'g', p+'b']
            if not columns_exist(h_names) and not columns_exist(r_names):
                pass

    if len(hsl_names)==3:
        H = df[df.columns[cols.index('clickcolh')]].to_numpy()
        S = df[df.columns[cols.index('clickcols')]].to_numpy()
        L = df[df.columns[cols.index('clickcoll')]].to_numpy()
        H1 = df[df.columns[cols.index('alt1colh')]].to_numpy()
        S1 = df[df.columns[cols.index('alt1cols')]].to_numpy()
        L1 = df[df.columns[cols.index('alt1coll')]].to_numpy()
        H2 = df[df.columns[cols.index('alt2colh')]].to_numpy()
        S2 = df[df.columns[cols.index('alt2cols')]].to_numpy()
        L2 = df[df.columns[cols.index('alt2coll')]].to_numpy()

        # normalize
        def norm_h(x):
            x = np.asarray(x, float)
            if x.max() > 2: x = x/360.0
            return (x % 1.0)
        def norm_sl(x):
            x = np.asarray(x, float)
            if x.max() > 2: x = x/100.0
            return np.clip(x, 0, 1)

        colors = np.stack([
            np.stack([norm_h(H), norm_sl(S), norm_sl(L)], axis=1),
            np.stack([norm_h(H1), norm_sl(S1), norm_sl(L1)], axis=1),
            np.stack([norm_h(H2), norm_sl(S2), norm_sl(L2)], axis=1),
        ], axis=1)  # (N,3,3)
    else:
        # try RGB (0..255)
        def g(name):
            return df[df.columns[cols.index(name)]].to_numpy()
        for name in ['clickr','clickg','clickb','alt1r','alt1g','alt1b','alt2r','alt2g','alt2b']:
            if name not in cols: raise SystemExit("RGB columns not found; need HSL columns (clickColH/S/L, alt1..., alt2...)")
        R0,G0,B0 = g('clickr'),g('clickg'),g('clickb')
        R1,G1,B1 = g('alt1r'),g('alt1g'),g('alt1b')
        R2,G2,B2 = g('alt2r'),g('alt2g'),g('alt2b')
        # naive RGB->HSL using matplotlib colors? avoid dependency; approximate hue from argmax channel
        def rgb_to_hsl(r,g,b):
            r,g,b = r/255.0, g/255.0, b/255.0
            mx, mn = max(r,g,b), min(r,g,b)
            l = (mx+mn)/2.0
            if mx==mn:
                h = 0.0; s = 0.0
            else:
                d = mx - mn
                s = d / (2.0 - mx - mn) if l > 0.5 else d / (mx + mn)
                if mx==r: h = ((g-b)/d + (6 if g<b else 0))/6.0
                elif mx==g: h = ((b-r)/d + 2)/6.0
                else: h = ((r-g)/d + 4)/6.0
            return h, s, l
        N = len(df)
        colors = np.zeros((N,3,3), float)
        for i,(r,g,b) in enumerate(zip(R0,G0,B0)):
            colors[i,0,:] = rgb_to_hsl(r,g,b)
        for i,(r,g,b) in enumerate(zip(R1,G1,B1)):
            colors[i,1,:] = rgb_to_hsl(r,g,b)
        for i,(r,g,b) in enumerate(zip(R2,G2,B2)):
            colors[i,2,:] = rgb_to_hsl(r,g,b)

    return colors, np.asarray(target_idx,int), cond

# -----------------------------
# Lexicon and scoring
# -----------------------------

def build_hue_lexicon(n_bins=12):
    """Return list of utterance prototypes on hue circle. Each utterance has center in [0,1)."""
    centers = [(i+0.5)/n_bins for i in range(n_bins)]
    return np.array(centers, float)

def l0_membership(hue, u_center, alpha=24.0):
    """
    Literal semantics: degree to which color with hue belongs to utterance u.
    alpha: sharpness; larger = narrower.
    """
    d = angular_dist(hue, u_center)
    return math.exp(-alpha * (d**2))

def ambiguity_mass(hues, u_center, alpha=24.0):
    """Ambiguity mass in the context: sum of memberships over referents."""
    return sum(l0_membership(h, u_center, alpha) for h in hues)

def compute_trial_decision(hsl_triplet, target_idx, params, setting_flags):
    """
    Compute model's chosen index and diagnostics for one trial.

    hsl_triplet: (3,3) H,S,L in [0,1]
    params: dict with keys beta, lam, alpha, kappa, eps, prior_mode ('uniform'|'salience')
    setting_flags: dict for ablations {A1:bool, A2:bool, A3:bool, A4:bool, A5:bool, A6:bool} where True means ON.
    """
    # Extract hues/salience proxies
    hues = hsl_triplet[:,0]
    sats = hsl_triplet[:,1]
    ligs = hsl_triplet[:,2]

    # Prior over referents
    if setting_flags.get('A2', True):
        prior = np.ones(3)/3.0  # uniform
    else:
        # salience-biased prior: proportional to saturation * (1 - |L-0.5|)  (distance from gray axis)
        sal = sats * (1.0 - np.abs(ligs - 0.5)*2.0)
        if sal.sum() <= 0: prior = np.ones(3)/3.0
        else: prior = sal/sal.sum()

    # L0 and IG surrogate per utterance center
    U = build_hue_lexicon(n_bins=params.get('n_lex', 12))
    alpha_eff = params['alpha'] * (1.0 if setting_flags.get('A5', True) else 1.0)  # here A5_OFF keeps alpha fixed; could randomize otherwise

    # For each target candidate t, compute best utterance utility
    scores = np.zeros(3)
    for t in range(3):
        best_u = -1; best_util = -1e9
        for uc in U:
            m_t = l0_membership(hues[t], uc, alpha=alpha_eff)
            # posterior if this u were uttered
            m_all = np.array([l0_membership(h, uc, alpha=alpha_eff) for h in hues])
            post = (m_all * prior); post = post/post.sum() if post.sum()>0 else np.ones(3)/3.0
            H_prior = -np.sum(prior*np.log(prior+1e-12))
            H_post = -np.sum(post*np.log(post+1e-12))
            IG = H_prior - H_post

            # utility parts
            inform = math.log(m_t + 1e-9)
            kappa = params['kappa'] if setting_flags.get('A1', True) else 0.0
            lam = params['lam'] if setting_flags.get('A3', True) else 0.0
            # ambiguity-dependent cost (A4)
            amb_cost = 0.0
            if setting_flags.get('A4', True):
                amb_mass = np.sum(m_all)
                amb_cost = params.get('eta', 0.2) * math.log(amb_mass + 1e-9)
            # length cost proxy: set to 1 token
            length_cost = lam * 1.0

            util = inform + kappa*IG - length_cost - amb_cost
            if util > best_util:
                best_util = util; best_u = uc
        scores[t] = best_util

    # Choice rule (A6): regular softmax vs epsilon-greedy
    if setting_flags.get('A6', True):
        beta = params['beta']
        logits = beta * scores
        # softmax
        ex = np.exp(logits - np.max(logits))
        probs = ex / ex.sum()
        choice = int(np.argmax(probs))
    else:
        # epsilon-greedy
        eps = params.get('eps', 0.2)
        if np.random.rand() < (1.0 - eps):
            choice = int(np.argmax(scores))
        else:
            # choose among the others uniformly
            others = [i for i in range(3) if i != int(np.argmax(scores))]
            choice = int(np.random.choice(others))

    # Diagnostics
    correct = int(choice == target_idx)
    # Define length proxy = 1 always; could tie to utterance bin width
    length = 1.0
    # IG proxy for chosen t: recompute at best u
    # (optional; here we approximate by recomputing with argmax u for chosen t)
    chosen_t = choice
    # find best u again to fetch IG
    best_u_for_choice = None; best_IG = 0.0
    for uc in U:
        m_t = l0_membership(hues[chosen_t], uc, alpha=alpha_eff)
        m_all = np.array([l0_membership(h, uc, alpha=alpha_eff) for h in hues])
        post = (m_all * prior); post = post/post.sum() if post.sum()>0 else np.ones(3)/3.0
        H_prior = -np.sum(prior*np.log(prior+1e-12))
        H_post = -np.sum(post*np.log(post+1e-12))
        IG = H_prior - H_post
        inform = math.log(m_t + 1e-9); lam = params['lam'] if setting_flags.get('A3', True) else 0.0
        amb_cost = 0.0
        if setting_flags.get('A4', True):
            amb_mass = np.sum(m_all)
            amb_cost = params.get('eta', 0.2) * math.log(amb_mass + 1e-9)
        util = inform + params['kappa']*IG - lam*1.0 - amb_cost
        if best_u_for_choice is None or util > best_IG:
            best_u_for_choice = uc; best_IG = IG

    return dict(correct=correct, length=length, IG=best_IG)

# -----------------------------
# Simulation
# -----------------------------

def run_ablation(colors, targets, conds, params):
    """
    Run ALL_ON, A1_OFF..A6_OFF across conditions.
    Returns: summary DataFrame and per-condition figures.
    """
    N = colors.shape[0]
    # ensure conditions set
    unique_conds = sorted(set(conds))
    if unique_conds == ['all']:
        unique_conds = ['all']

    settings = [('ALL_ON', dict(A1=True,A2=True,A3=True,A4=True,A5=True,A6=True))]
    for k in range(1,6):
        d = dict(A1=True,A2=True,A3=True,A4=True,A5=True,A6=True)
        d[f"A{k}"] = False
        settings.append((f"A{k}_OFF", d))
    # Add A6_OFF too
    d = dict(A1=True,A2=True,A3=True,A4=True,A5=True,A6=False)
    settings.append(("A6_OFF", d))

    rows = []
    # Iterate conditions
    for cond in unique_conds:
        idxs = np.arange(N) if cond=='all' else np.where(np.array(conds)==cond)[0]
        if len(idxs)==0: continue
        for name, flags in settings:
            succ = 0; lengths = []; IGs = []
            for j in idxs:
                res = compute_trial_decision(colors[j], int(targets[j]), params, flags)
                succ += int(res['correct']==1)
                lengths.append(res['length']); IGs.append(res['IG'])
            p, lo, hi = wilson_from_counts(succ, len(idxs))
            avg_len, se_len = mean_and_se(lengths)
            avg_ig, se_ig = mean_and_se(IGs)
            rows.append(dict(condition=cond, setting=name, n=len(idxs), successes=succ,
                             accuracy=p, acc_lo=lo, acc_hi=hi, avg_len=avg_len, len_se=se_len,
                             avg_IG=avg_ig, ig_se=se_ig))
    return pd.DataFrame(rows)

def barplot_a1to5(df, cond, outpath):
    sub = df[(df['condition']==cond) & (df['setting'].isin(['ALL_ON','A1_OFF','A2_OFF','A3_OFF','A4_OFF','A5_OFF']))]
    order = ['ALL_ON','A1_OFF','A2_OFF','A3_OFF','A4_OFF','A5_OFF']
    sub = sub.set_index('setting').loc[order].reset_index()
    y = sub['accuracy'].values
    yerr = np.vstack([y - sub['acc_lo'].values, sub['acc_hi'].values - y])
    xs = np.arange(len(order))
    plt.figure(figsize=(6,4))
    plt.bar(xs, y)
    plt.errorbar(xs, y, yerr=yerr, fmt='none', capsize=4)
    plt.xticks(xs, order, rotation=0)
    plt.ylim(0,1)
    plt.ylabel("Accuracy (95% CI)")
    plt.title(f"Ablations A1â€“A5 ({cond})")
    plt.tight_layout(); plt.savefig(outpath, dpi=160); plt.close()

def barplot_a6_only(df, outpath):
    sub = df[df['setting'].isin(['ALL_ON','A6_OFF'])].copy()
    pooled = sub.groupby('setting', as_index=False)[['successes','n']].sum()
    order = ['ALL_ON','A6_OFF']
    pooled = pooled.set_index('setting').loc[order].reset_index()
    y = (pooled['successes']/pooled['n']).values
    # compute Wilson from counts
    y_ci = np.array([wilson_from_counts(int(s), int(n)) for s,n in pooled[['successes','n']].values])
    ylo = y_ci[:,1]; yhi = y_ci[:,2]
    yerr = np.vstack([y - ylo, yhi - y])
    xs = np.arange(2)
    plt.figure(figsize=(5.5,4))
    plt.bar(xs, y)
    plt.errorbar(xs, y, yerr=yerr, fmt='none', capsize=4)
    plt.xticks(xs, order)
    plt.ylim(0,1)
    plt.ylabel("Accuracy (pooled, 95% CI)")
    plt.title("A6 regular vs. non-regular")
    plt.tight_layout(); plt.savefig(outpath, dpi=160); plt.close()

def barplot_a1to5_pooled(df, outpath):
    sub = df[df['setting'].isin(['ALL_ON','A1_OFF','A2_OFF','A3_OFF','A4_OFF','A5_OFF'])].copy()
    pooled = sub.groupby('setting', as_index=False)[['successes','n']].sum()
    order = ['ALL_ON','A1_OFF','A2_OFF','A3_OFF','A4_OFF','A5_OFF']
    pooled = pooled.set_index('setting').loc[order].reset_index()
    y = (pooled['successes']/pooled['n']).values
    y_ci = np.array([wilson_from_counts(int(s), int(n)) for s,n in pooled[['successes','n']].values])
    ylo = y_ci[:,1]; yhi = y_ci[:,2]
    yerr = np.vstack([y - ylo, yhi - y])
    xs = np.arange(len(order))
    plt.figure(figsize=(6,4))
    plt.bar(xs, y)
    plt.errorbar(xs, y, yerr=yerr, fmt='none', capsize=4)
    plt.xticks(xs, order, rotation=0)
    plt.ylim(0,1)
    plt.ylabel("Accuracy (pooled, 95% CI)")
    plt.title("Ablations A1â€“A5 (pooled)")
    plt.tight_layout(); plt.savefig(outpath, dpi=160); plt.close()

def sensitivity_grid(colors, targets, params, betas=(4,8,16), lambdas=(0.3,0.6,0.9)):
    rows=[]
    # pool all samples (user request: use all samples)
    N = colors.shape[0]
    for b in betas:
        for lam in lambdas:
            succ=0; lens=[]; IGs=[]
            p2 = params.copy(); p2['beta']=b; p2['lam']=lam
            flags = dict(A1=True,A2=True,A3=True,A4=True,A5=True,A6=True)  # ALL_ON during sensitivity
            for j in range(N):
                res = compute_trial_decision(colors[j], int(targets[j]), p2, flags)
                succ += int(res['correct']==1); lens.append(res['length']); IGs.append(res['IG'])
            acc, lo, hi = wilson_from_counts(succ, N)
            Lm, Lse = mean_and_se(lens); IGm, IGse = mean_and_se(IGs)
            rows.append(dict(beta=b, lam=lam, n=N, accuracy=acc, acc_lo=lo, acc_hi=hi,
                             avg_len=Lm, len_se=Lse, avg_IG=IGm, ig_se=IGse))
    return pd.DataFrame(rows)

def heatmap_sensitivity(df, outpath):
    # build grid
    betas = sorted(df['beta'].unique())
    lams = sorted(df['lam'].unique())
    acc = np.zeros((len(betas), len(lams)))
    for i,b in enumerate(betas):
        for j,L in enumerate(lams):
            acc[i,j] = df[(df['beta']==b)&(df['lam']==L)]['accuracy'].values[0]
    plt.figure(figsize=(6,4))
    plt.imshow(acc, aspect='auto', origin='lower')
    plt.xticks(range(len(lams)), [f"{x:.1f}" for x in lams])
    plt.yticks(range(len(betas)), [f"{int(x)}" for x in betas])
    plt.xlabel(r"$\lambda$"); plt.ylabel(r"$\beta$")
    plt.title("Sensitivity (accuracy)")
    plt.colorbar()
    plt.tight_layout(); plt.savefig(outpath, dpi=160); plt.close()

# -----------------------------
# Tables (LaTeX)
# -----------------------------

def write_table1(df, path_tex):
    # Table 1: Ablation across conditions (ALL_ON + A1..A6)
    lines = []
    lines += [r"\begin{table}[!htbp]", r"\centering", r"\small"]
    lines += [r"\caption{Ablation summary across conditions and settings (all samples). Accuracy reports Wilson 95\% CIs; length and IG are mean $\pm$ SE.}"]
    lines += [r"\label{tab:ablation}", r"\begin{tabular}{llrlll}", r"\toprule"]
    lines += [r"Condition & Setting & $n$ & Accuracy [95\% CI] & Length (mean $\pm$ SE) & IG (mean $\pm$ SE) \\",
              r"\midrule"]
    # order conditions: close, split, far, then 'all'
    cond_order = ['close','split','far','all']
    set_order = ['ALL_ON','A1_OFF','A2_OFF','A3_OFF','A4_OFF','A5_OFF','A6_OFF']
    df2 = df.copy()
    df2['cond_order'] = df2['condition'].apply(lambda c: cond_order.index(c) if c in cond_order else 9)
    df2['set_order'] = df2['setting'].apply(lambda s: set_order.index(s) if s in set_order else 99)
    df2 = df2.sort_values(['cond_order','set_order'])
    for _, r in df2.iterrows():
        acc = f"{r['accuracy']:.3f} [{r['acc_lo']:.3f}, {r['acc_hi']:.3f}]"
        L = f"{r['avg_len']:.3f} $\\pm$ {r['len_se']:.3f}"
        IG = f"{r['avg_IG']:.3f} $\\pm$ {r['ig_se']:.3f}"
        lines += [f"{r['condition']} & {r['setting']} & {int(r['n'])} & {acc} & {L} & {IG} \\\\"]
    lines += [r"\bottomrule", r"\end{tabular}", r"\end{table}"]
    pathlib.Path(path_tex).write_text("\n".join(lines), encoding="utf-8")

def write_table2(df, path_tex):
    # Table 2: Sensitivity grid pooled over all samples
    lines = []
    lines += [r"\begin{table}[!htbp]", r"\centering", r"\small"]
    lines += [r"\caption{Sensitivity over $\beta$ (inverse temperature) and $\lambda$ (length cost). Accuracy shows Wilson 95\% CI; length and IG are mean $\pm$ SE pooled over all samples.}"]
    lines += [r"\label{tab:sensitivity}", r"\begin{tabular}{ccrlll}", r"\toprule"]
    lines += [r"$\beta$ & $\lambda$ & $n$ & Accuracy [95\% CI] & Length (mean $\pm$ SE) & IG (mean $\pm$ SE) \\",
              r"\midrule"]
    df2 = df.sort_values(['beta','lam'])
    for _, r in df2.iterrows():
        acc = f"{r['accuracy']:.3f} [{r['acc_lo']:.3f}, {r['acc_hi']:.3f}]"
        L = f"{r['avg_len']:.3f} $\\pm$ {r['len_se']:.3f}"
        IG = f"{r['avg_IG']:.3f} $\\pm$ {r['ig_se']:.3f}"
        lines += [f"{int(r['beta'])} & {r['lam']:.1f} & {int(r['n'])} & {acc} & {L} & {IG} \\\\"]
    lines += [r"\bottomrule", r"\end{tabular}", r"\end{table}"]
    pathlib.Path(path_tex).write_text("\n".join(lines), encoding="utf-8")

# -----------------------------
# Main
# -----------------------------

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--csv", required=True, help="Path to filteredCorpus.csv")
    ap.add_argument("--out_dir", default="output")
    ap.add_argument("--beta", type=float, default=8.0)
    ap.add_argument("--lambda_cost", type=float, default=0.6)
    ap.add_argument("--alpha", type=float, default=24.0)  # semantic sharpness
    ap.add_argument("--kappa", type=float, default=0.3)   # IG weight
    ap.add_argument("--eps", type=float, default=0.2)     # epsilon for non-regular
    ap.add_argument("--eta", type=float, default=0.2)     # ambiguity cost weight
    ap.add_argument("--betas", default="4,8,16")          # for sensitivity
    ap.add_argument("--lambdas", default="0.3,0.6,0.9")   # for sensitivity
    args = ap.parse_args()

    out = pathlib.Path(args.out_dir); (out/"figures").mkdir(parents=True, exist_ok=True); (out/"tables").mkdir(parents=True, exist_ok=True)

    # Load all samples
    colors, targets, conds = load_cic_csv(args.csv)
    print(f"[load] samples: {len(targets)} conditions: {sorted(set(conds))}")

    # Params
    params = dict(beta=args.beta, lam=args.lambda_cost, alpha=args.alpha, kappa=args.kappa, eps=args.eps, eta=args.eta, n_lex=12)

    # Run ablation on full dataset (no subsampling; user requested to use ALL samples)
    summary = run_ablation(colors, targets, conds, params)
    summary.to_csv(out/"figures"/"summary.csv", index=False)
    print("[out] summary.csv ->", out/"figures"/"summary.csv")

    # Figures 1â€“3: per-condition A1â€“A5 bars (+ALL_ON). If conditions not present, generate just one and duplicate.
    conds_unique = sorted(set(conds))
    if conds_unique == ['all']:
        conds_plot = ['all','all','all']
    else:
        # ensure order close/split/far if available
        order = ['close','split','far']
        conds_plot = [c for c in order if c in conds_unique]
        # pad if missing
        while len(conds_plot) < 3: conds_plot.append(conds_plot[-1] if conds_plot else 'all')

    barplot_a1to5(summary, conds_plot[0], out/"figures"/"figure1.png")
    barplot_a1to5(summary, conds_plot[1], out/"figures"/"figure2.png")
    barplot_a1to5(summary, conds_plot[2], out/"figures"/"figure3.png")

    # Figure 4: A6-only pooled
    barplot_a6_only(summary, out/"figures"/"figure4.png")

    # Figure 6: pooled A1â€“A5
    barplot_a1to5_pooled(summary, out/"figures"/"figure6.png")

    # Table 1
    write_table1(summary, out/"tables"/"table1.tex")

    # Sensitivity (Table 2 + Figure 5 heatmap)
    betas = tuple(float(x) for x in args.betas.split(","))
    lambdas = tuple(float(x) for x in args.lambdas.split(","))
    sens = sensitivity_grid(colors, targets, params, betas=betas, lambdas=lambdas)
    sens.to_csv(out/"figures"/"sensitivity.csv", index=False)
    write_table2(sens, out/"tables"/"table2.tex")
    heatmap_sensitivity(sens, out/"figures"/"figure5.png")

    print("[done] Figures 1â€“6 and Tables 1â€“2 written under", out)

if __name__ == "__main__":
    main()
